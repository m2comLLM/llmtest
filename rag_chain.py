"""LlamaIndex RAG ì²´ì¸ - ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì§€ì›."""

import re
from datetime import datetime, timedelta
from typing import Generator

from llama_index.core import PromptTemplate, Settings
from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator
from llama_index.llms.ollama import Ollama

import config
from vector_store import get_index, get_all_by_filter
from embeddings import get_embed_model


def get_today_korean() -> str:
    """Get today's date in Korean format."""
    today = datetime.now()
    return f"{today.year}ë…„ {today.month}ì›” {today.day}ì¼"

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_llm: Ollama | None = None

# í˜ì´ì§€ë„¤ì´ì…˜ ìƒíƒœ
_last_search_results: list = []
_last_search_offset: int = 0
_last_search_query: str = ""

def get_system_prompt() -> str:
    """Get system prompt with current date."""
    today = get_today_korean()
    today_date = datetime.now().strftime("%Y-%m-%d")
    return f"""ë‹¹ì‹ ì€ ì‚¬ë‚´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ê¸°ì¤€ ì •ë³´
- ì˜¤ëŠ˜ ë‚ ì§œ: {today} ({today_date})
- ì´ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê³¼ê±°/ë¯¸ë˜, ë“±ë¡ ê°€ëŠ¥ ì—¬ë¶€ ë“±ì„ íŒë‹¨í•˜ì„¸ìš”.

## í•„ìˆ˜ ê·œì¹™
1. ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.
2. ê²€ìƒ‰ëœ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
3. ì •ë³´ê°€ ì—†ìœ¼ë©´ "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
4. í–‰ì‚¬ëª…, ê³ ìœ ëª…ì‚¬, ì¥ì†Œëª… ë“±ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.

## ë“±ë¡ ìƒíƒœ íŒë‹¨ ê¸°ì¤€ (ì˜¤ëŠ˜: {today_date})
- "ë“±ë¡ ê°€ëŠ¥": ì˜¤ëŠ˜ì´ ë“±ë¡ì‹œì‘ì¼ê³¼ ë“±ë¡ë§ˆê°ì¼ ì‚¬ì´
- "ë§ˆê° ì„ë°•": ë“±ë¡ë§ˆê°ì¼ì´ 7ì¼ ì´ë‚´
- "ë“±ë¡ ì „": ë“±ë¡ì‹œì‘ì¼ì´ ì˜¤ëŠ˜ ì´í›„
- "ë§ˆê°ë¨": ë“±ë¡ë§ˆê°ì¼ì´ ì˜¤ëŠ˜ ì´ì „

## ë‹µë³€ í˜•ì‹
- ì—¬ëŸ¬ í•­ëª©: ë²ˆí˜¸ ë§¤ê²¨ì„œ ë¹ ì§ì—†ì´ ì „ë¶€ ë‚˜ì—´
- í‘œ ìš”ì²­ ì‹œ: Markdown í‘œ í˜•ì‹ (| ì»¬ëŸ¼1 | ì»¬ëŸ¼2 |)
- URL ìˆìœ¼ë©´: í•¨ê»˜ ì œê³µ
- ë“±ë¡ê¸°ê°„ ìˆìœ¼ë©´: í•¨ê»˜ ì•ˆë‚´

## ì¤‘ìš”: í•„í„°ë§ëœ ê²°ê³¼ ì²˜ë¦¬
- ì œê³µëœ ë¬¸ì„œëŠ” ì´ë¯¸ ì§ˆë¬¸ ì¡°ê±´ì— ë§ê²Œ í•„í„°ë§ëœ ê²°ê³¼ì…ë‹ˆë‹¤.
- ì‚¬ìš©ìê°€ "ë“±ë¡ ê°€ëŠ¥í•œ" ë“± ë“±ë¡ ìƒíƒœë¥¼ ëª…ì‹œí•˜ì§€ ì•Šì•˜ë‹¤ë©´, ë“±ë¡ ë§ˆê° ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ëª¨ë“  ë¬¸ì„œë¥¼ ë‹µë³€ì— í¬í•¨í•˜ì„¸ìš”.
- ë“±ë¡ìƒíƒœëŠ” ì°¸ê³  ì •ë³´ì¼ ë¿, ë‹µë³€ì—ì„œ ì œì™¸í•˜ëŠ” ê¸°ì¤€ì´ ì•„ë‹™ë‹ˆë‹¤.

## ëª¨í˜¸í•œ ì§ˆë¬¸ ì²˜ë¦¬
- "ê·¸ê±°", "ê±°ê¸°" ë“± ëŒ€ìƒì´ ë¶ˆëª…í™•í•˜ë©´ ë˜ë¬¼ì–´ë³´ì„¸ìš”.
- ì˜ˆ: "ì–´ë–¤ í–‰ì‚¬ë¥¼ ë§ì”€í•˜ì‹œëŠ” ê±´ê°€ìš”?"
"""

def get_qa_prompt() -> PromptTemplate:
    """Get QA prompt with current date."""
    today = get_today_korean()
    return PromptTemplate(
        f"""\
ì˜¤ëŠ˜ ë‚ ì§œ: {today}

ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:

{{context_str}}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
- ì—¬ëŸ¬ í•­ëª©ì´ ìˆìœ¼ë©´ ì „ë¶€ ë‚˜ì—´í•˜ì„¸ìš”.
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
- "ì˜¤ëŠ˜", "ê°€ì¥ ë¹ ë¥¸" ë“±ì€ ì˜¤ëŠ˜ ë‚ ì§œ({today}) ê¸°ì¤€ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {{query_str}}

ë‹µë³€:"""
    )


def get_llm() -> Ollama:
    """Get the Ollama LLM instance (singleton)."""
    global _llm

    if _llm is None:
        print("[ì´ˆê¸°í™”] LLM ì—°ê²° ì¤‘...")
        _llm = Ollama(
            model=config.OLLAMA_MODEL,
            base_url=config.OLLAMA_BASE_URL,
            request_timeout=120.0,
        )
        print("[ì´ˆê¸°í™”] LLM ì—°ê²° ì™„ë£Œ")

    return _llm


def setup_settings() -> None:
    """Configure global LlamaIndex settings."""
    Settings.llm = get_llm()
    Settings.embed_model = get_embed_model()
    Settings.chunk_size = config.CHUNK_SIZE
    Settings.chunk_overlap = config.CHUNK_OVERLAP


def parse_date_from_query(query: str) -> tuple[int | None, int | None, list[int] | None]:
    """Extract year, month, and month range from query string.

    Returns:
        (year, month, month_range) - month_range is a list of months for range queries
    """
    year = None
    month = None
    month_range = None

    # ì—°ë„ íŒŒì‹±
    year_match = re.search(r"(\d{4})ë…„", query)
    if year_match:
        year = int(year_match.group(1))

    # ê¸°ê°„ í‚¤ì›Œë“œ íŒŒì‹± (ìƒë°˜ê¸°, í•˜ë°˜ê¸°, ë¶„ê¸° ë“±)
    if re.search(r"ìƒë°˜ê¸°|1ë°˜ê¸°|ì „ë°˜ê¸°", query):
        month_range = [1, 2, 3, 4, 5, 6]
    elif re.search(r"í•˜ë°˜ê¸°|2ë°˜ê¸°|í›„ë°˜ê¸°", query):
        month_range = [7, 8, 9, 10, 11, 12]
    elif re.search(r"1ë¶„ê¸°|1ì‚¬ë¶„ê¸°", query):
        month_range = [1, 2, 3]
    elif re.search(r"2ë¶„ê¸°|2ì‚¬ë¶„ê¸°", query):
        month_range = [4, 5, 6]
    elif re.search(r"3ë¶„ê¸°|3ì‚¬ë¶„ê¸°", query):
        month_range = [7, 8, 9]
    elif re.search(r"4ë¶„ê¸°|4ì‚¬ë¶„ê¸°", query):
        month_range = [10, 11, 12]

    # ëª…ì‹œì  ì›” ë²”ìœ„ íŒŒì‹± (ì˜ˆ: "1ì›”~6ì›”", "3ì›”ë¶€í„° 5ì›”ê¹Œì§€", "1ì›”-6ì›”")
    range_match = re.search(r"(\d{1,2})ì›”\s*[~\-ë¶€í„°]\s*(\d{1,2})ì›”", query)
    if range_match:
        start_month = int(range_match.group(1))
        end_month = int(range_match.group(2))
        if 1 <= start_month <= 12 and 1 <= end_month <= 12 and start_month <= end_month:
            month_range = list(range(start_month, end_month + 1))

    # ë‹¨ì¼ ì›” íŒŒì‹± (ë²”ìœ„ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
    if month_range is None:
        month_match = re.search(r"(\d{1,2})ì›”", query)
        if month_match:
            month = int(month_match.group(1))

    return year, month, month_range


def parse_category_from_query(query: str) -> str | None:
    """Extract event category from query string."""
    category_patterns = [
        (r"ì‹¬í¬ì§€ì—„|ì‹¬í¬ì§€ì›€", "ì‹¬í¬ì§€ì—„"),
        (r"ì›Œí¬ìˆ|ì›Œí¬ìƒµ", "ì›Œí¬ìˆ"),
        (r"ìŠ¤ì¿¨|school", "ìŠ¤ì¿¨"),
        (r"í•™ìˆ ëŒ€íšŒ", "í•™ìˆ ëŒ€íšŒ"),
        (r"êµìœ¡|ì—°ìˆ˜|ë¦¬ë”ì‰½", "êµìœ¡"),
        (r"ì„¸ë¯¸ë‚˜", "ì„¸ë¯¸ë‚˜"),
    ]

    query_lower = query.lower()
    for pattern, category in category_patterns:
        if re.search(pattern, query_lower):
            return category

    return None


def parse_credits_from_query(query: str) -> tuple[int | None, str | None]:
    """Extract credits (í‰ì ) info from query string.

    Returns:
        (credit_value, organization) - e.g., (4, "ëŒ€í•œì˜ì‚¬í˜‘íšŒ")
    """
    credit_value = None
    organization = None

    # í‰ì  ìˆ«ì íŒŒì‹± (4ì , 4í‰ì  ëª¨ë‘ ì¸ì‹)
    credit_match = re.search(r"(\d+)\s*(?:í‰ì |ì )", query)
    if credit_match:
        credit_value = int(credit_match.group(1))

    # ê¸°ê´€ëª… íŒŒì‹±
    org_patterns = [
        (r"ëŒ€í•œì˜ì‚¬í˜‘íšŒ|ì˜ì‚¬í˜‘íšŒ|ì˜í˜‘", "ëŒ€í•œì˜ì‚¬í˜‘íšŒ"),
        (r"ë‚´ê³¼ë¶„ê³¼|ë‚´ê³¼", "ë‚´ê³¼ë¶„ê³¼"),
        (r"ëŒ€í•œì„ìƒë³‘ë¦¬ì‚¬í˜‘íšŒ|ì„ìƒë³‘ë¦¬ì‚¬", "ëŒ€í•œì„ìƒë³‘ë¦¬ì‚¬í˜‘íšŒ"),
    ]
    for pattern, org_name in org_patterns:
        if re.search(pattern, query):
            organization = org_name
            break

    return credit_value, organization


def filter_nodes_by_credits(nodes: list, credit_value: int | None, organization: str | None) -> list:
    """Filter nodes by credits (Python post-processing)."""
    if credit_value is None and organization is None:
        return nodes

    filtered = []
    for node in nodes:
        metadata = node.metadata if hasattr(node, 'metadata') else {}
        credits = metadata.get("credits", "")

        # í‰ì  ê°’ í™•ì¸
        if credit_value is not None:
            if f"{credit_value}í‰ì " not in credits and credits != str(credit_value):
                continue

        # ê¸°ê´€ëª… í™•ì¸
        if organization is not None:
            if organization not in credits:
                continue

        filtered.append(node)

    return filtered


def parse_location_from_query(query: str) -> str | None:
    """Extract location keyword from query string."""
    location_patterns = [
        (r"ì–‘ì¬\s*aT\s*ì„¼í„°", "ì–‘ì¬ aTì„¼í„°"),
        (r"ì„œìš¸ëŒ€", "ì„œìš¸ëŒ€"),
        (r"ì½”ì—‘ìŠ¤", "ì½”ì—‘ìŠ¤"),
        (r"ë²¡ìŠ¤ì½”", "ë²¡ìŠ¤ì½”"),
        (r"SC\s*ì»¨ë²¤ì…˜", "SC ì»¨ë²¤ì…˜ì„¼í„°"),
        (r"ì„±ëª¨ë³‘ì›", "ì„±ëª¨ë³‘ì›"),
        (r"ì¤‘ì•™ëŒ€", "ì¤‘ì•™ëŒ€"),
    ]

    for pattern, normalized in location_patterns:
        if re.search(pattern, query, re.IGNORECASE):
            return normalized

    return None


def parse_weekend_filter(query: str) -> bool | None:
    """Extract weekend/weekday filter from query string.

    Returns:
        True: ì£¼ë§ë§Œ
        False: í‰ì¼ë§Œ
        None: í•„í„° ì—†ìŒ
    """
    if re.search(r"ì£¼ë§|í† ìš”ì¼|ì¼ìš”ì¼|í† ,?\s*ì¼|í† Â·ì¼", query):
        return True
    if re.search(r"í‰ì¼|ì›”ìš”ì¼|í™”ìš”ì¼|ìˆ˜ìš”ì¼|ëª©ìš”ì¼|ê¸ˆìš”ì¼|ì›”~ê¸ˆ", query):
        return False
    return None


def parse_registration_filter(query: str) -> str | None:
    """ë“±ë¡ ìƒíƒœ í‚¤ì›Œë“œ íŒŒì‹±.

    Returns:
        "available": í˜„ì¬ ë“±ë¡ ê°€ëŠ¥
        "closing_soon": ë§ˆê° ì„ë°• (7ì¼ ì´ë‚´)
        "upcoming": ë“±ë¡ ì‹œì‘ ì „
        "exclude_closed": ë§ˆê°ëœ ê²ƒ ì œì™¸
        None: í•„í„° ì—†ìŒ
    """
    # "ë“±ë¡"ì´ í¬í•¨ëœ ê²½ìš°ì—ë§Œ ë“±ë¡ ìƒíƒœ í•„í„° ì ìš©
    if re.search(r"ë“±ë¡.*(ê°€ëŠ¥|ì‹ ì²­|ì ‘ìˆ˜)|ì§€ê¸ˆ.*(ì‹ ì²­|ë“±ë¡)|ë‹¹ì¥.*(ì‹ ì²­|ë“±ë¡)", query):
        return "available"
    if re.search(r"ë“±ë¡.*(ë§ˆê°|ì„ë°•)|ë§ˆê°.*(ì„ë°•|ê¸‰|ê³§)|ì¼ì£¼ì¼.*(ì•ˆ|ë‚´).*ë§ˆê°", query):
        return "closing_soon"
    if re.search(r"ë“±ë¡.*(ì „|ëŒ€ê¸°|ì‹œì‘.*ì „)|ì•„ì§.*ë“±ë¡.*(ì•ˆ|ì „)", query):
        return "upcoming"
    if re.search(r"ë“±ë¡.*(ë§ˆê°|ë|ì§€ë‚œ).*(ì œì™¸|ë¹¼)|ë§ˆê°.*ì œì™¸", query):
        return "exclude_closed"
    return None


def parse_duration_filter(query: str) -> str | None:
    """í–‰ì‚¬ ê¸°ê°„ í•„í„° íŒŒì‹±.

    Returns:
        "multi_day": ë©°ì¹ ê°„ ì§„í–‰
        "single_day": í•˜ë£¨ í–‰ì‚¬
        None: í•„í„° ì—†ìŒ
    """
    if re.search(r"ë©°ì¹ |ì—¬ëŸ¬\s*ë‚ |ì¥ê¸°|ì´í‹€|[23]ì¼ê°„|ì—°ì†|ë™ì•ˆ\s*ì§„í–‰", query):
        return "multi_day"
    if re.search(r"í•˜ë£¨|ë‹¹ì¼|ë‹¨ê¸°|í•˜ë£¨\s*ë§Œ", query):
        return "single_day"
    return None


def is_pagination_request(query: str) -> bool:
    """Check if query is asking for more results."""
    patterns = [
        r"ë”\s*(ë³´ì—¬|ì•Œë ¤|ì¤˜)",
        r"ì¶”ê°€ë¡œ\s*(ë³´ì—¬|ì•Œë ¤|ì¤˜)",
        r"ë‹¤ìŒ\s*(ëª©ë¡|í˜ì´ì§€|ê²°ê³¼)",
        r"ë‚˜ë¨¸ì§€",
        r"ë”\s*ìˆ",
        r"ê³„ì†",
        r"ë‹¤ìŒìœ¼ë¡œ",
        r"ì¶”ê°€\s*ëª©ë¡",
        r"ë”\s*ë§ì´",
    ]
    for pattern in patterns:
        if re.search(pattern, query):
            return True
    return False


def parse_exclusion_filter(query: str) -> str | None:
    """ì œì™¸ ì¡°ê±´ íŒŒì‹± (ì¹´í…Œê³ ë¦¬).

    Returns:
        ì œì™¸í•  ì¹´í…Œê³ ë¦¬ëª… ë˜ëŠ” None
    """
    patterns = [
        (r"ì‹¬í¬ì§€ì—„.*(ë§ê³ |ì œì™¸|ë¹¼ê³ |ì•„ë‹ˆê³ |ì™¸)", "ì‹¬í¬ì§€ì—„"),
        (r"ì›Œí¬ìˆ.*(ë§ê³ |ì œì™¸|ë¹¼ê³ |ì•„ë‹ˆê³ |ì™¸)", "ì›Œí¬ìˆ"),
        (r"ìŠ¤ì¿¨.*(ë§ê³ |ì œì™¸|ë¹¼ê³ |ì•„ë‹ˆê³ |ì™¸)", "ìŠ¤ì¿¨"),
        (r"ì„¸ë¯¸ë‚˜.*(ë§ê³ |ì œì™¸|ë¹¼ê³ |ì•„ë‹ˆê³ |ì™¸)", "ì„¸ë¯¸ë‚˜"),
        (r"êµìœ¡.*(ë§ê³ |ì œì™¸|ë¹¼ê³ |ì•„ë‹ˆê³ |ì™¸)", "êµìœ¡"),
    ]
    for pattern, category in patterns:
        if re.search(pattern, query):
            return category
    return None


def build_metadata_filters(query: str) -> MetadataFilters | None:
    """Build metadata filters from query string (for LlamaIndex)."""
    filters = []

    # ì—°ë„ í•„í„°
    year, month, month_range = parse_date_from_query(query)
    if year:
        filters.append(MetadataFilter(key="year", value=year, operator=FilterOperator.EQ))
    if month:
        filters.append(MetadataFilter(key="month", value=month, operator=FilterOperator.EQ))
    # ì°¸ê³ : LlamaIndex MetadataFilterëŠ” $in ë¯¸ì§€ì›, month_rangeëŠ” ChromaDB ì§ì ‘ í•„í„°ì—ì„œë§Œ ì‚¬ìš©

    # ì¹´í…Œê³ ë¦¬ í•„í„°
    category = parse_category_from_query(query)
    if category:
        filters.append(MetadataFilter(key="category", value=category, operator=FilterOperator.EQ))

    if filters:
        return MetadataFilters(filters=filters)
    return None


def is_time_based_query(query: str) -> bool:
    """Check if query is asking about upcoming/nearest events."""
    time_patterns = [
        r"ê°€ì¥\s*ë¹ ë¥¸",
        r"ê°€ì¥\s*ë¹¨ë¦¬",
        r"ê°€ì¥\s*ê°€ê¹Œìš´",
        r"ì˜¤ëŠ˜\s*ì´í›„",
        r"ë‚´ì¼\s*ì´í›„",
        r"ë‹¤ìŒ\s*í–‰ì‚¬",
        r"ê°€ê¹Œìš´\s*í–‰ì‚¬",
        r"ë‹¤ê°€ì˜¤ëŠ”",
        r"ì˜ˆì •ëœ",
        r"ê³§\s*ìˆëŠ”",
        r"ì•ìœ¼ë¡œ",
        r"ì˜¤ëŠ˜\s*ê¸°ì¤€",
        r"ì´ë²ˆ\s*ë‹¬",
        r"ì´ë²ˆ\s*ì£¼",
    ]
    for pattern in time_patterns:
        if re.search(pattern, query):
            return True
    return False


def build_chroma_filters(query: str) -> dict | None:
    """Build ChromaDB where clause from query string."""
    conditions = []

    # ì—°ë„/ì›”/ì›”ë²”ìœ„ íŒŒì‹±
    year, month, month_range = parse_date_from_query(query)

    # ì‹œê°„ ê¸°ë°˜ ì¿¼ë¦¬ ì²˜ë¦¬ (ì˜¤ëŠ˜ ì´í›„ í–‰ì‚¬ë§Œ)
    if is_time_based_query(query):
        today = datetime.now()
        today_int = int(today.strftime("%Y%m%d"))

        # ì‚¬ìš©ìê°€ ëª…ì‹œí•œ ì—°ë„/ì›”ì´ ê³¼ê±°ì¸ì§€ í™•ì¸
        is_past_date = False
        if year and month:
            query_date_int = int(f"{year}{month:02d}28")
            is_past_date = query_date_int < today_int
        elif year and month_range:
            # ì›” ë²”ìœ„ì˜ ë§ˆì§€ë§‰ ì›” ê¸°ì¤€
            last_month = max(month_range)
            query_date_int = int(f"{year}{last_month:02d}28")
            is_past_date = query_date_int < today_int
        elif year:
            is_past_date = year < today.year

        # ê³¼ê±° ë‚ ì§œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ "ì˜¤ëŠ˜ ì´í›„" í•„í„° ì ìš©
        if not is_past_date:
            conditions.append({"start_date_int": {"$gte": today_int}})

    # ì—°ë„ í•„í„°
    if year:
        conditions.append({"year": {"$eq": year}})

    # ì›” í•„í„° (ë‹¨ì¼ ë˜ëŠ” ë²”ìœ„)
    if month_range:
        conditions.append({"month": {"$in": month_range}})
    elif month:
        conditions.append({"month": {"$eq": month}})

    # ì£¼ë§/í‰ì¼ í•„í„°
    is_weekend = parse_weekend_filter(query)
    if is_weekend is not None:
        conditions.append({"is_weekend": {"$eq": is_weekend}})

    # ì¹´í…Œê³ ë¦¬ í•„í„°
    category = parse_category_from_query(query)
    if category:
        conditions.append({"category": {"$eq": category}})

    # ì œì™¸ ì¡°ê±´ ($ne)
    exclusion = parse_exclusion_filter(query)
    if exclusion:
        conditions.append({"category": {"$ne": exclusion}})

    # ë“±ë¡ ìƒíƒœ í•„í„°
    today_int = int(datetime.now().strftime("%Y%m%d"))
    reg_status = parse_registration_filter(query)
    if reg_status == "available":
        # ì˜¤ëŠ˜ì´ ë“±ë¡ ê¸°ê°„ ë‚´ (ì‹œì‘ì¼ <= ì˜¤ëŠ˜ <= ë§ˆê°ì¼)
        conditions.append({"reg_start_int": {"$lte": today_int}})
        conditions.append({"reg_end_int": {"$gte": today_int}})
    elif reg_status == "closing_soon":
        # ë§ˆê° 7ì¼ ì´ë‚´
        week_later = int((datetime.now() + timedelta(days=7)).strftime("%Y%m%d"))
        conditions.append({"reg_end_int": {"$gte": today_int}})
        conditions.append({"reg_end_int": {"$lte": week_later}})
    elif reg_status == "upcoming":
        # ë“±ë¡ ì‹œì‘ ì „
        conditions.append({"reg_start_int": {"$gt": today_int}})
    elif reg_status == "exclude_closed":
        # ë§ˆê° ì•ˆ ëœ ê²ƒë§Œ
        conditions.append({"reg_end_int": {"$gte": today_int}})

    # í–‰ì‚¬ ê¸°ê°„ í•„í„°
    duration = parse_duration_filter(query)
    if duration == "multi_day":
        conditions.append({"duration_days": {"$gt": 1}})
    elif duration == "single_day":
        conditions.append({"duration_days": {"$eq": 1}})

    # ì°¸ê³ : ì¥ì†Œ í•„í„°ëŠ” ChromaDBì—ì„œ $contains ë¯¸ì§€ì›ìœ¼ë¡œ Python í›„ì²˜ë¦¬ì—ì„œ ìˆ˜í–‰

    if not conditions:
        return None
    elif len(conditions) == 1:
        return conditions[0]
    else:
        return {"$and": conditions}


def filter_nodes_by_location(nodes: list, location: str) -> list:
    """Filter nodes by location (Python post-processing)."""
    if not location:
        return nodes

    filtered = []
    for node in nodes:
        metadata = node.metadata if hasattr(node, 'metadata') else {}
        node_location = metadata.get("location", "")
        if location in node_location:
            filtered.append(node)

    return filtered


def build_filter_description(query: str) -> str:
    """ì ìš©ëœ í•„í„°ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì„¤ëª…."""
    descriptions = []

    year, month, month_range = parse_date_from_query(query)
    if year:
        descriptions.append(f"{year}ë…„")
    if month:
        descriptions.append(f"{month}ì›”")
    if month_range:
        descriptions.append(f"{min(month_range)}ì›”~{max(month_range)}ì›”")

    is_weekend = parse_weekend_filter(query)
    if is_weekend is True:
        descriptions.append("ì£¼ë§(í† /ì¼) í–‰ì‚¬")
    elif is_weekend is False:
        descriptions.append("í‰ì¼ í–‰ì‚¬")

    category = parse_category_from_query(query)
    if category:
        descriptions.append(f"ì¹´í…Œê³ ë¦¬: {category}")

    exclusion = parse_exclusion_filter(query)
    if exclusion:
        descriptions.append(f"{exclusion} ì œì™¸")

    reg_status = parse_registration_filter(query)
    if reg_status == "available":
        descriptions.append("í˜„ì¬ ë“±ë¡ ê°€ëŠ¥")
    elif reg_status == "closing_soon":
        descriptions.append("ë“±ë¡ ë§ˆê° ì„ë°•")
    elif reg_status == "upcoming":
        descriptions.append("ë“±ë¡ ì‹œì‘ ì „")

    duration = parse_duration_filter(query)
    if duration == "multi_day":
        descriptions.append("ë©°ì¹ ê°„ ì§„í–‰ í–‰ì‚¬")
    elif duration == "single_day":
        descriptions.append("ë‹¹ì¼ í–‰ì‚¬")

    location = parse_location_from_query(query)
    if location:
        descriptions.append(f"ì¥ì†Œ: {location}")

    credit_value, credit_org = parse_credits_from_query(query)
    if credit_value is not None:
        descriptions.append(f"í‰ì : {credit_value}ì ")
    if credit_org is not None:
        descriptions.append(f"ì¸ì •ê¸°ê´€: {credit_org}")

    if is_time_based_query(query):
        descriptions.append("ì˜¤ëŠ˜ ì´í›„ í–‰ì‚¬")

    if descriptions:
        return f"[ì ìš©ëœ í•„í„°: {', '.join(descriptions)}]"
    return ""


def sort_nodes_by_date(nodes: list, ascending: bool = True) -> list:
    """Sort nodes by start_date_int."""
    def get_date_int(node):
        metadata = node.metadata if hasattr(node, 'metadata') else {}
        return metadata.get("start_date_int", 99999999)  # ë‚ ì§œ ì—†ìœ¼ë©´ ë§¨ ë’¤ë¡œ

    return sorted(nodes, key=get_date_int, reverse=not ascending)


def calculate_registration_status(metadata: dict) -> str:
    """ë“±ë¡ ìƒíƒœë¥¼ ì‹¤ì‹œê°„ ê³„ì‚°."""
    today_int = int(datetime.now().strftime("%Y%m%d"))
    reg_start = metadata.get("reg_start_int")
    reg_end = metadata.get("reg_end_int")

    if not reg_start or not reg_end:
        return ""

    if today_int < reg_start:
        return "ë“±ë¡ìƒíƒœ: ë“±ë¡ ì‹œì‘ ì „"
    elif today_int <= reg_end:
        days_left = (reg_end % 100) - (today_int % 100)  # ê°„ë‹¨í•œ ì¼ìˆ˜ ê³„ì‚°
        if reg_end // 100 != today_int // 100:  # ì›”ì´ ë‹¤ë¥´ë©´ ëŒ€ëµ ê³„ì‚°
            days_left = "ë©°ì¹ "
        if isinstance(days_left, int) and days_left <= 7:
            return f"ë“±ë¡ìƒíƒœ: ë“±ë¡ ê°€ëŠ¥ (ë§ˆê° {days_left}ì¼ ì „)"
        return "ë“±ë¡ìƒíƒœ: ë“±ë¡ ê°€ëŠ¥"
    else:
        return "ë“±ë¡ìƒíƒœ: ë“±ë¡ ë§ˆê°"


def _handle_pagination_request(message: str) -> str:
    """Handle request for more results from previous search."""
    global _last_search_results, _last_search_offset, _last_search_query

    max_docs = config.RETRIEVAL_K
    total_count = len(_last_search_results)

    # ë‹¤ìŒ í˜ì´ì§€ ê³„ì‚°
    start_idx = _last_search_offset
    end_idx = min(start_idx + max_docs, total_count)

    if start_idx >= total_count:
        return f"ë” ì´ìƒ í‘œì‹œí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (ì´ {total_count}ê°œ ëª¨ë‘ í‘œì‹œë¨)"

    # ë‹¤ìŒ í˜ì´ì§€ ë…¸ë“œ ê°€ì ¸ì˜¤ê¸°
    page_nodes = _last_search_results[start_idx:end_idx]
    display_count = len(page_nodes)

    # ì˜¤í”„ì…‹ ì—…ë°ì´íŠ¸
    _last_search_offset = end_idx

    # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë²ˆí˜¸ëŠ” ì „ì²´ ê¸°ì¤€ìœ¼ë¡œ)
    context = format_nodes_as_context(page_nodes, start_number=start_idx + 1)

    llm = get_llm()
    remaining = total_count - end_idx

    prompt = f"""ë‹¤ìŒì€ ì´ì „ ê²€ìƒ‰ ê²°ê³¼ì˜ ì¶”ê°€ ëª©ë¡ì…ë‹ˆë‹¤.

[{start_idx + 1}ë²ˆ ~ {end_idx}ë²ˆ / ì´ {total_count}ê°œ]

{context}

ìœ„ ëª©ë¡ì„ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•´ì„œ ë³´ì—¬ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
{f'(ì•„ì§ {remaining}ê°œ ë” ìˆìŠµë‹ˆë‹¤. "ë” ë³´ì—¬ì¤˜"ë¡œ í™•ì¸ ê°€ëŠ¥)' if remaining > 0 else '(ë§ˆì§€ë§‰ í˜ì´ì§€ì…ë‹ˆë‹¤)'}

ë‹µë³€:"""

    response = llm.complete(prompt)
    result = str(response)

    if remaining > 0:
        result += f"\n\n---\nğŸ“„ {remaining}ê°œì˜ ê²°ê³¼ê°€ ë” ìˆìŠµë‹ˆë‹¤. 'ë” ë³´ì—¬ì¤˜'ë¡œ í™•ì¸í•˜ì„¸ìš”."

    return result


def format_nodes_as_context(nodes: list, max_nodes: int | None = None, start_number: int = 1) -> str:
    """Format nodes as concise context string for LLM."""
    if max_nodes:
        nodes = nodes[:max_nodes]

    context_parts = []
    for i, node in enumerate(nodes, start_number):
        metadata = node.metadata if hasattr(node, 'metadata') else {}

        # ê°„ê²°í•œ í¬ë§· ì‚¬ìš©
        answer = metadata.get("answer_template", "")
        url = metadata.get("url", "")

        # ë“±ë¡ ìƒíƒœ ì‹¤ì‹œê°„ ê³„ì‚°
        reg_status = calculate_registration_status(metadata)

        if answer:
            entry = f"{i}. {answer}"
            if reg_status:
                entry += f"\n   {reg_status}"
            if url:
                entry += f"\n   URL: {url}"
            context_parts.append(entry)
        else:
            text = node.text if hasattr(node, 'text') else str(node)
            context_parts.append(f"{i}. {text[:300]}")

    return "\n\n".join(context_parts)


def chat(message: str) -> str:
    """
    Process a chat message and return the response.

    Args:
        message: User message

    Returns:
        AI response string
    """
    global _last_search_results, _last_search_offset, _last_search_query

    setup_settings()

    # í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì²­ ì²˜ë¦¬
    if is_pagination_request(message) and _last_search_results:
        return _handle_pagination_request(message)

    # ì¿¼ë¦¬ì—ì„œ í•„í„° ì¶”ì¶œ
    chroma_filters = build_chroma_filters(message)

    # ì¥ì†Œ í•„í„° ì¶”ì¶œ (Python í›„ì²˜ë¦¬ìš©)
    location = parse_location_from_query(message)

    # í‰ì  í•„í„° ì¶”ì¶œ (Python í›„ì²˜ë¦¬ìš©)
    credit_value, credit_org = parse_credits_from_query(message)

    if chroma_filters or location or credit_value is not None or credit_org is not None:
        if chroma_filters:
            # í•„í„°ê°€ ìˆìœ¼ë©´ ChromaDBì—ì„œ ëª¨ë“  ë§¤ì¹­ ë¬¸ì„œ ì§ì ‘ ì¡°íšŒ
            nodes = get_all_by_filter(chroma_filters)
            print(f"[ê²€ìƒ‰] í•„í„° ì ìš©: {chroma_filters}, ê²°ê³¼: {len(nodes)}ê°œ (ì „ì²´)")
        else:
            # ì¥ì†Œ í•„í„°ë§Œ ìˆëŠ” ê²½ìš° ì „ì²´ ë¬¸ì„œì—ì„œ í•„í„°ë§
            nodes = get_all_by_filter(None)
            print(f"[ê²€ìƒ‰] ì „ì²´ ë¬¸ì„œ ì¡°íšŒ: {len(nodes)}ê°œ")

        # ì¥ì†Œ í•„í„° (Python í›„ì²˜ë¦¬ - ChromaDBëŠ” $contains ë¯¸ì§€ì›)
        if location:
            nodes = filter_nodes_by_location(nodes, location)
            print(f"[í•„í„°] ì¥ì†Œ í•„í„° ì ìš©: {location}, ê²°ê³¼: {len(nodes)}ê°œ")

        # í‰ì  í•„í„° (Python í›„ì²˜ë¦¬)
        if credit_value is not None or credit_org is not None:
            nodes = filter_nodes_by_credits(nodes, credit_value, credit_org)
            print(f"[í•„í„°] í‰ì  í•„í„° ì ìš©: {credit_value}í‰ì , {credit_org}, ê²°ê³¼: {len(nodes)}ê°œ")

        if not nodes:
            _last_search_results = []
            _last_search_offset = 0
            _last_search_query = ""
            return "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ì‹œê°„ ê¸°ë°˜ ì¿¼ë¦¬ë©´ ë‚ ì§œìˆœ ì •ë ¬
        if is_time_based_query(message):
            nodes = sort_nodes_by_date(nodes, ascending=True)
            print(f"[ì •ë ¬] ë‚ ì§œìˆœ ì •ë ¬ ì™„ë£Œ")

        # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ (í˜ì´ì§€ë„¤ì´ì…˜ìš©)
        _last_search_results = nodes
        _last_search_query = message
        _last_search_offset = config.RETRIEVAL_K  # ì²« í˜ì´ì§€ ì´í›„ë¶€í„° ì‹œì‘

        # ë¬¸ì„œ ìˆ˜ ì œí•œ (LLM ì†ë„ ìµœì í™”)
        max_docs = config.RETRIEVAL_K  # ê¸°ë³¸ 20ê°œ
        display_count = min(max_docs, len(nodes))
        total_count = len(nodes)
        context = format_nodes_as_context(nodes, max_nodes=max_docs)
        print(f"[LLM] {display_count}ê°œ ë¬¸ì„œ ì „ë‹¬ (ì´ {total_count}ê°œ)")

        # ì ìš©ëœ í•„í„° ì„¤ëª… ìƒì„±
        filter_desc = build_filter_description(message)

        llm = get_llm()
        system_prompt = get_system_prompt()
        prompt = f"""{system_prompt}

{filter_desc}
ë‹¤ìŒì€ ì§ˆë¬¸ ì¡°ê±´ì— ë§ëŠ” ë¬¸ì„œ {total_count}ê°œ ì¤‘ {display_count}ê°œì…ë‹ˆë‹¤:

{context}

ìœ„ ë¬¸ì„œë“¤ì€ ì´ë¯¸ ì§ˆë¬¸ ì¡°ê±´ì— ë§ê²Œ í•„í„°ë§ëœ ê²°ê³¼ì…ë‹ˆë‹¤.
ì´ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ì—¬ëŸ¬ ê°œë©´ ì „ë¶€ ë‚˜ì—´í•˜ì„¸ìš”.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: {message}

ë‹µë³€:"""

        response = llm.complete(prompt)
        result = str(response)

        # ì¶”ê°€ ê²°ê³¼ ì•ˆë‚´
        remaining = total_count - display_count
        if remaining > 0:
            result += f"\n\n---\nğŸ“„ {remaining}ê°œì˜ ê²°ê³¼ê°€ ë” ìˆìŠµë‹ˆë‹¤. 'ë” ë³´ì—¬ì¤˜'ë¡œ í™•ì¸í•˜ì„¸ìš”."

        return result

    else:
        # í•„í„°ê°€ ì—†ìœ¼ë©´ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‚¬ìš©
        index = get_index()
        query_engine = index.as_query_engine(
            similarity_top_k=config.RETRIEVAL_K,
            text_qa_template=get_qa_prompt(),
            system_prompt=get_system_prompt(),
        )

        response = query_engine.query(message)

        if hasattr(response, 'source_nodes'):
            print(f"[ê²€ìƒ‰] ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼: {len(response.source_nodes)}ê°œ")

        return str(response)


def reset_chat_engine() -> None:
    """Reset function (placeholder for compatibility)."""
    global _last_search_results, _last_search_offset, _last_search_query
    _last_search_results = []
    _last_search_offset = 0
    _last_search_query = ""
    print("[ì´ˆê¸°í™”] ChatEngine ë¦¬ì…‹ ì™„ë£Œ")
